{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "plt.style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import china_cities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from transliterate import translit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_DIR.joinpath(\"train.csv\"), index_col=\"pair_id\")\n",
    "test_df = pd.read_csv(DATA_DIR.joinpath(\"test.csv\"), index_col=\"pair_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.copy()\n",
    "test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [country.name.lower() for country in pycountry.countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [city.lower() for city in china_cities.get_cities_en()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces = [province.lower() for province in china_cities.get_provinces()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"name_1\"] = train[\"name_1\"].str.lower()\n",
    "train[\"name_2\"] = train[\"name_2\"].str.lower()\n",
    "\n",
    "test[\"name_1\"] = test[\"name_1\"].str.lower()\n",
    "test[\"name_2\"] = test[\"name_2\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_str_replace(strings, debug=False):\n",
    "    re_str = r'\\b(?:' + '|'.join(\n",
    "        [re.escape(s) for s in strings]\n",
    "    ) + r')(?!\\S)'\n",
    "    if debug:\n",
    "        print(re_str)\n",
    "    return re.compile(re_str, re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_entities = [\"ltd\\.\", \"co\\.\", \"inc\\.\", \"b\\.v\\.\", \"s\\.c\\.r\\.l\\.\", \"gmbh\", \"pvt\\.\", \"llc\", \"corp\", \"corp\\.\",\n",
    "                  \"bv\", \"s\\.a\\.\", \"c\\.v\\.\", \"ltda\", \"de\", \"cv\", \"sa\", \"ca\", \"c\\.a\\.\", \"ооо\", \"ooo\", \"гк\"]\n",
    "legal_re = re.compile(r'\\s*\\b(?:' + '|'.join([rf\"{entity}\" for entity in legal_entities]) + r')(?!\\S)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in (train, test):\n",
    "    dataset.replace(to_replace=re.compile(r\"\\s+\\(.*\\)\"), value=\"\", inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_re = multi_str_replace(countries)\n",
    "cities_re = multi_str_replace(cities)\n",
    "provinces_re = multi_str_replace(provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train, test :\n",
    "    dataset.replace(to_replace=countries_re, value=\"\", inplace=True, regex=True)\n",
    "    dataset.replace(to_replace=cities_re, value=\"\", inplace=True, regex=True)\n",
    "    dataset.replace(to_replace=provinces_re, value=\"\", inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in (train, test):\n",
    "    dataset.replace(to_replace=re.compile(r\"[^\\w\\s]\"), value=\" \", inplace=True, regex=True)\n",
    "    dataset.replace(to_replace=re.compile(r\"\\s+\"), value=\" \", inplace=True, regex=True)\n",
    "    dataset.replace(to_replace=re.compile(r\"\\s*\\b\\w{1}\\b\"), value=\"\", inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = []\n",
    "for row in train[train['is_duplicate'] == 0].iterrows():\n",
    "    name_1 = row[1]['name_1']\n",
    "    name_2 = row[1]['name_2']\n",
    "    i = set(name_1.split()) & set(name_2.split())\n",
    "    for w in i:\n",
    "        intersections.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_re = multi_str_replace(set(intersections))\n",
    "for dataset in (train, test):\n",
    "    dataset.replace(to_replace=junk_re, value=\"\", inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерирование фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strsimpy.levenshtein import Levenshtein\n",
    "from strsimpy.qgram import QGram\n",
    "from strsimpy.jaro_winkler import JaroWinkler\n",
    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name_1_tr'] = train['name_1'].apply(lambda x: translit(x, 'ru'))\n",
    "train['name_2_tr'] = train['name_2'].apply(lambda x: translit(x, 'ru'))\n",
    "\n",
    "test['name_1_tr'] = test['name_1'].apply(lambda x: translit(x, 'ru'))\n",
    "test['name_2_tr'] = test['name_2'].apply(lambda x: translit(x, 'ru'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['n_intersect'] = train.apply(lambda x: len(set(x['name_1'].split()) & set(x['name_2'].split())), axis=1)\n",
    "test['n_intersect'] = test.apply(lambda x: len(set(x['name_1'].split()) & set(x['name_2'].split())), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['coltdetc_n1'] = train_df['name_1'].apply(lambda x: len(re.findall(legal_re, x.lower())))\n",
    "test['coltdetc_n1'] = test_df['name_1'].apply(lambda x: len(re.findall(legal_re, x.lower())))\n",
    "\n",
    "train['coltdetc_n2'] = train_df['name_2'].apply(lambda x: len(re.findall(legal_re, x.lower())))\n",
    "test['coltdetc_n2'] = test_df['name_2'].apply(lambda x: len(re.findall(legal_re, x.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein = Levenshtein()\n",
    "\n",
    "train[\"levenshtein\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2), axis=1)\n",
    "test[\"levenshtein\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2), axis=1)\n",
    "\n",
    "train[\"levenshtein_tr21\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2_tr), axis=1)\n",
    "test[\"levenshtein_tr21\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_1, r.name_2_tr), axis=1) \n",
    "\n",
    "train[\"levenshtein_tr12\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_2, r.name_1_tr), axis=1)\n",
    "test[\"levenshtein_tr12\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_2, r.name_1_tr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_levenshtein = NormalizedLevenshtein()\n",
    "\n",
    "train[\"norm_levenshtein\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2),\n",
    "                                                axis=1)\n",
    "test[\"norm_levenshtein\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2),\n",
    "                                              axis=1)\n",
    "train[\"norm_levenshtein_tr21\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2_tr), axis=1)\n",
    "test[\"norm_levenshtein_tr21\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1, r.name_2_tr), axis=1) \n",
    "\n",
    "train[\"norm_levenshtein_tr12\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_2, r.name_1_tr), axis=1)\n",
    "test[\"norm_levenshtein_tr12\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_2, r.name_1_tr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"levenshtein_cont\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_1.replace(' ', ''), r.name_2.replace(' ', '')), axis=1)\n",
    "test[\"levenshtein_cont\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_1.replace(' ', ''), r.name_2.replace(' ', '')), axis=1)\n",
    "\n",
    "train[\"levenshtein_tr21_cont\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_1.replace(' ', ''), r.name_2_tr.replace(' ', '')), axis=1)\n",
    "test[\"levenshtein_tr21_cont\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_1.replace(' ', ''), r.name_2_tr.replace(' ', '')), axis=1) \n",
    "\n",
    "train[\"levenshtein_tr12_cont\"] = train.progress_apply(lambda r: levenshtein.distance(r.name_2.replace(' ', ''), r.name_1_tr.replace(' ', '')), axis=1)\n",
    "test[\"levenshtein_tr12_cont\"] = test.progress_apply(lambda r: levenshtein.distance(r.name_2.replace(' ', ''), r.name_1_tr.replace(' ', '')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"norm_levenshtein_cont\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1.replace(' ', ''), r.name_2.replace(' ', '')),\n",
    "                                                axis=1)\n",
    "test[\"norm_levenshtein_cont\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1.replace(' ', ''), r.name_2.replace(' ', '')),\n",
    "                                              axis=1)\n",
    "train[\"norm_levenshtein_tr21_cont\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1.replace(' ', ''), r.name_2_tr.replace(' ', '')), axis=1)\n",
    "test[\"norm_levenshtein_tr21_cont\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_1.replace(' ', ''), r.name_2_tr.replace(' ', '')), axis=1) \n",
    "\n",
    "train[\"norm_levenshtein_tr12_cont\"] = train.progress_apply(lambda r: normalized_levenshtein.distance(r.name_2.replace(' ', ''), r.name_1_tr.replace(' ', '')), axis=1)\n",
    "test[\"norm_levenshtein_tr12_cont\"] = test.progress_apply(lambda r: normalized_levenshtein.distance(r.name_2.replace(' ', ''), r.name_1_tr.replace(' ', '')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgram = QGram()\n",
    "\n",
    "train[\"qgram\"] = train.progress_apply(lambda r: qgram.distance(r.name_1, r.name_2), axis=1)\n",
    "test[\"qgram\"] = test.progress_apply(lambda r: qgram.distance(r.name_1, r.name_2), axis=1)\n",
    "\n",
    "train[\"qgram_tr21\"] = train.progress_apply(lambda r: qgram.distance(r.name_1, r.name_2_tr), axis=1)\n",
    "test[\"qgram_tr21\"] = test.progress_apply(lambda r: qgram.distance(r.name_1, r.name_2_tr), axis=1) \n",
    "\n",
    "train[\"qgram_tr12\"] = train.progress_apply(lambda r: qgram.distance(r.name_2, r.name_1_tr), axis=1)\n",
    "test[\"qgram_tr12\"] = test.progress_apply(lambda r: qgram.distance(r.name_2, r.name_1_tr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_upper(name_string):\n",
    "    counter = 0\n",
    "    for c in name_string:\n",
    "        if c.isupper():\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['upper_count1'] = train_df['name_1'].apply(count_upper)\n",
    "train['upper_count2'] = train_df['name_2'].apply(count_upper)\n",
    "\n",
    "test['upper_count1'] = test_df['name_1'].apply(count_upper)\n",
    "test['upper_count2'] = test_df['name_2'].apply(count_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_punct1'] = train_df['name_1'].apply(lambda x: len(re.findall(r'[^\\s\\w]', x)))\n",
    "train['num_punct2'] = train_df['name_2'].apply(lambda x: len(re.findall(r'[^\\s\\w]', x)))\n",
    "\n",
    "test['num_punct1'] = test_df['name_1'].apply(lambda x: len(re.findall(r'[^\\s\\w]', x)))\n",
    "test['num_punct2'] = test_df['name_2'].apply(lambda x: len(re.findall(r'[^\\s\\w]', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SequenceMatcher()\n",
    "def get_ratios(seq1, seq2):\n",
    "    sm.set_seqs(seq1, seq2)\n",
    "    return sm.ratio()\n",
    "\n",
    "def get_list_ratios(seq1, seq2):\n",
    "    sm.set_seqs(seq1.split(), seq2.split())\n",
    "    return sm.ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ratios'] = train.apply(lambda x: get_ratios(x['name_1'], x['name_2']), axis=1)\n",
    "test['ratios'] = test.apply(lambda x: get_ratios(x['name_1'], x['name_2']), axis=1)\n",
    "\n",
    "train['ratios_tr12'] = train.apply(lambda x: get_ratios(x['name_1_tr'], x['name_2']), axis=1)\n",
    "test['ratios_tr12'] = test.apply(lambda x: get_ratios(x['name_1_tr'], x['name_2']), axis=1)\n",
    "\n",
    "train['ratios_tr21'] = train.apply(lambda x: get_ratios(x['name_1'], x['name_2_tr']), axis=1)\n",
    "test['ratios_tr21'] = test.apply(lambda x: get_ratios(x['name_1'], x['name_2_tr']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ratios_cont'] = train.apply(lambda x: get_ratios(x['name_1'].replace(' ', ''), x['name_2'].replace(' ', '')), axis=1)\n",
    "test['ratios_cont'] = test.apply(lambda x: get_ratios(x['name_1'].replace(' ', ''), x['name_2'].replace(' ', '')), axis=1)\n",
    "\n",
    "train['ratios_tr12_cont'] = train.apply(lambda x: get_ratios(x['name_1_tr'].replace(' ', ''), x['name_2'].replace(' ', '')), axis=1)\n",
    "test['ratios_tr12_cont'] = test.apply(lambda x: get_ratios(x['name_1_tr'].replace(' ', ''), x['name_2'].replace(' ', '')), axis=1)\n",
    "\n",
    "train['ratios_tr21_cont'] = train.apply(lambda x: get_ratios(x['name_1'].replace(' ', ''), x['name_2_tr'].replace(' ', '')), axis=1)\n",
    "test['ratios_tr21_cont'] = test.apply(lambda x: get_ratios(x['name_1'].replace(' ', ''), x['name_2_tr'].replace(' ', '')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['seq_ratios'] = train.apply(lambda x: get_list_ratios(x['name_1'], x['name_2']), axis=1)\n",
    "test['seq_ratios'] = test.apply(lambda x: get_list_ratios(x['name_1'], x['name_2']), axis=1)\n",
    "\n",
    "train['seq_ratios_tr12'] = train.apply(lambda x: get_list_ratios(x['name_1_tr'], x['name_2']), axis=1)\n",
    "test['seq_ratios_tr12'] = test.apply(lambda x: get_list_ratios(x['name_1_tr'], x['name_2']), axis=1)\n",
    "\n",
    "train['seq_ratios_tr21'] = train.apply(lambda x: get_list_ratios(x['name_1'], x['name_2_tr']), axis=1)\n",
    "test['seq_ratios_tr21'] = test.apply(lambda x: get_list_ratios(x['name_1'], x['name_2_tr']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name1_len'] = train['name_1'].apply(len)\n",
    "train['name2_len'] = train['name_2'].apply(len)\n",
    "\n",
    "test['name1_len'] = test['name_1'].apply(len)\n",
    "test['name2_len'] = test['name_2'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name1_nwords'] = train['name_1'].apply(lambda x: len(x.split()))\n",
    "train['name2_nwords'] = train['name_2'].apply(lambda x: len(x.split()))\n",
    "\n",
    "test['name1_nwords'] = test['name_1'].apply(lambda x: len(x.split()))\n",
    "test['name2_nwords'] = test['name_2'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['first_w_inter'] = train.apply(lambda x: 1 if all((len(x['name_1'].split()), len(x['name_2'].split()))) and x['name_1'].split()[0] == x['name_2'].split()[0] else 0, axis=1)\n",
    "test['first_w_inter'] = test.apply(lambda x: 1 if all((len(x['name_1'].split()), len(x['name_2'].split()))) and x['name_1'].split()[0] == x['name_2'].split()[0] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_n1 = max(train['name1_nwords'])\n",
    "longest_n2 = max(train['name2_nwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_lev_feats = []\n",
    "test_words_lev_feats = []\n",
    "\n",
    "for i, row in enumerate(train.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].split()\n",
    "    n2 = row[1]['name_2'].split()\n",
    "    \n",
    "    for j in range(longest_n1):\n",
    "        for k in range(longest_n2):\n",
    "            if j >= len(n1):\n",
    "                w_row.append(0)\n",
    "            elif k >= len(n2):\n",
    "                w_row.append(0)\n",
    "            else:\n",
    "                w_row.append(levenshtein.distance(n1[j], n2[k]))\n",
    "    train_words_lev_feats.append(w_row)\n",
    "    \n",
    "for i, row in enumerate(test.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].split()\n",
    "    n2 = row[1]['name_2'].split()\n",
    "    \n",
    "    for j in range(longest_n1):\n",
    "        for k in range(longest_n2):\n",
    "            if j >= len(n1):\n",
    "                w_row.append(0)\n",
    "            elif k >= len(n2):\n",
    "                w_row.append(0)\n",
    "            else:\n",
    "                w_row.append(levenshtein.distance(n1[j], n2[k]))\n",
    "    test_words_lev_feats.append(w_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_lev_feats = sp.coo_matrix(train_words_lev_feats)\n",
    "test_sparse_lev_feats = sp.coo_matrix(test_words_lev_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_norm_lev_feats = []\n",
    "test_words_norm_lev_feats = []\n",
    "\n",
    "for i, row in enumerate(train.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].split()\n",
    "    n2 = row[1]['name_2'].split()\n",
    "    \n",
    "    for j in range(longest_n1):\n",
    "        for k in range(longest_n2):\n",
    "            if j >= len(n1):\n",
    "                w_row.append(0)\n",
    "            elif k >= len(n2):\n",
    "                w_row.append(0)\n",
    "            else:\n",
    "                w_row.append(normalized_levenshtein.distance(n1[j], n2[k]))\n",
    "    train_words_norm_lev_feats.append(w_row)\n",
    "    \n",
    "for i, row in enumerate(test.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].split()\n",
    "    n2 = row[1]['name_2'].split()\n",
    "    \n",
    "    for j in range(longest_n1):\n",
    "        for k in range(longest_n2):\n",
    "            if j >= len(n1):\n",
    "                w_row.append(0)\n",
    "            elif k >= len(n2):\n",
    "                w_row.append(0)\n",
    "            else:\n",
    "                w_row.append(normalized_levenshtein.distance(n1[j], n2[k]))\n",
    "    test_words_norm_lev_feats.append(w_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_norm_lev_feats = sp.coo_matrix(train_words_norm_lev_feats)\n",
    "test_sparse_norm_lev_feats = sp.coo_matrix(test_words_norm_lev_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_ratios_feats = []\n",
    "test_words_ratios_feats = []\n",
    "\n",
    "for i, row in enumerate(train.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].split()\n",
    "    n2 = row[1]['name_2'].split()\n",
    "    \n",
    "    for j in range(longest_n1):\n",
    "        for k in range(longest_n2):\n",
    "            if j >= len(n1):\n",
    "                w_row.append(0)\n",
    "            elif k >= len(n2):\n",
    "                w_row.append(0)\n",
    "            else:\n",
    "                sm.set_seqs(n1[j], n2[k])\n",
    "                w_row.append(sm.ratio())\n",
    "    train_words_ratios_feats.append(w_row)\n",
    "    \n",
    "for i, row in enumerate(test.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].split()\n",
    "    n2 = row[1]['name_2'].split()\n",
    "    \n",
    "    for j in range(longest_n1):\n",
    "        for k in range(longest_n2):\n",
    "            if j >= len(n1):\n",
    "                w_row.append(0)\n",
    "            elif k >= len(n2):\n",
    "                w_row.append(0)\n",
    "            else:\n",
    "                sm.set_seqs(n1[j], n2[k])\n",
    "                w_row.append(sm.ratio())\n",
    "    test_words_ratios_feats.append(w_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_ratio_feats = sp.coo_matrix(train_words_ratios_feats)\n",
    "test_sparse_ratio_feats = sp.coo_matrix(test_words_ratios_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['intersections'] = train.apply(lambda x: ' '.join(set(x['name_1'].split()) & set(x['name_2'].split())), axis=1)\n",
    "test['intersections'] = test.apply(lambda x: ' '.join(set(x['name_1'].split()) & set(x['name_2'].split())), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['feat1'] = train['seq_ratios'] + train['norm_levenshtein']\n",
    "train['feat2'] = train['ratios_cont'] + train['norm_levenshtein']\n",
    "train['feat3'] = train['qgram'] + train['levenshtein']\n",
    "train['feat4'] = train_df.apply(lambda x: 1 if all((len(x['name_1'].split()), len(x['name_2'].split()))) and x['name_1'].split()[0].lower() == x['name_2'].split()[0].lower() else 0, axis=1)\n",
    "\n",
    "test['feat1'] = test['seq_ratios'] + test['norm_levenshtein']\n",
    "test['feat2'] = test['ratios_cont'] + test['norm_levenshtein']\n",
    "test['feat3'] = test['qgram'] + test['levenshtein']\n",
    "test['feat4'] = test_df.apply(lambda x: 1 if all((len(x['name_1'].split()), len(x['name_2'].split()))) and x['name_1'].split()[0].lower() == x['name_2'].split()[0].lower() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['feat5'] = train.apply(lambda x: 1 if all((len(x['name_1_tr'].split()), len(x['name_2'].split()))) and x['name_1_tr'].split()[0].lower() == x['name_2'].split()[0].lower() else 0, axis=1)\n",
    "train['feat6'] = train.apply(lambda x: 1 if all((len(x['name_1'].split()), len(x['name_2_tr'].split()))) and x['name_1'].split()[0].lower() == x['name_2_tr'].split()[0].lower() else 0, axis=1)\n",
    "\n",
    "test['feat5'] = test.apply(lambda x: 1 if all((len(x['name_1_tr'].split()), len(x['name_2'].split()))) and x['name_1_tr'].split()[0].lower() == x['name_2'].split()[0].lower() else 0, axis=1)\n",
    "test['feat6'] = test.apply(lambda x: 1 if all((len(x['name_1'].split()), len(x['name_2_tr'].split()))) and x['name_1'].split()[0].lower() == x['name_2_tr'].split()[0].lower() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name1_nwords_before'] = train_df['name_1'].apply(lambda x: len(x.split()))\n",
    "train['name2_nwords_before'] = train_df['name_2'].apply(lambda x: len(x.split()))\n",
    "\n",
    "longest_n1_before = max(train['name1_nwords_before'])\n",
    "longest_n2_before = max(train['name2_nwords_before'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_junk_feats = []\n",
    "test_words_junk_feats = []\n",
    "sint = set(intersections)\n",
    "\n",
    "for i, row in enumerate(train_df.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].lower().split()\n",
    "    n2 = row[1]['name_2'].lower().split()\n",
    "    \n",
    "    for j in range(longest_n1_before):\n",
    "        if j >= len(n1):\n",
    "            w_row.append(0)\n",
    "        elif n1[j] in sint:\n",
    "            w_row.append(1)\n",
    "        else:\n",
    "            w_row.append(0)\n",
    "            \n",
    "    for k in range(longest_n2_before):\n",
    "        if k >= len(n2):\n",
    "            w_row.append(0)\n",
    "        elif n2[k] in sint:\n",
    "            w_row.append(1)\n",
    "        else:\n",
    "            w_row.append(0)\n",
    "            \n",
    "    train_words_junk_feats.append(w_row)\n",
    "    \n",
    "for i, row in enumerate(test_df.iterrows()):\n",
    "    w_row = []\n",
    "    n1 = row[1]['name_1'].lower().split()\n",
    "    n2 = row[1]['name_2'].lower().split()\n",
    "    \n",
    "    for j in range(longest_n1_before):\n",
    "        if j >= len(n1):\n",
    "            w_row.append(0)\n",
    "        elif n1[j] in sint:\n",
    "            w_row.append(1)\n",
    "        else:\n",
    "            w_row.append(0)\n",
    "            \n",
    "    for k in range(longest_n2_before):\n",
    "        if k >= len(n2):\n",
    "            w_row.append(0)\n",
    "        elif n2[k] in sint:\n",
    "            w_row.append(1)\n",
    "        else:\n",
    "            w_row.append(0)\n",
    "            \n",
    "    test_words_junk_feats.append(w_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_junk_feats = sp.coo_matrix(train_words_junk_feats)\n",
    "test_sparse_junk_feats = sp.coo_matrix(test_words_junk_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внешние данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_external = pd.read_csv(DATA_DIR.joinpath(\"train_external.csv\"), index_col=\"pair_id\")\n",
    "test_external = pd.read_csv(DATA_DIR.joinpath(\"test_external.csv\"), index_col=\"pair_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat((train, train_external), axis=1)\n",
    "test = pd.concat((test, test_external), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit(train[['industry_n1', 'industry_n2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTS = [\"first_w_inter\", \"norm_levenshtein\", \"ratios\", \"seq_ratios\", \"name1_len\", \"name2_len\", \"name1_nwords\",\n",
    "       \"name2_nwords\", \"levenshtein_tr21\", \"levenshtein_tr12\", \"norm_levenshtein_tr21\", \"norm_levenshtein_tr12\",\n",
    "       \"ratios_tr12\", \"ratios_tr21\", \"seq_ratios_tr12\", \"seq_ratios_tr21\", \"levenshtein_cont\",\n",
    "       \"levenshtein_tr21_cont\", \"levenshtein_tr12_cont\", \"norm_levenshtein_cont\", \"norm_levenshtein_tr21_cont\",\n",
    "       \"norm_levenshtein_tr12_cont\", \"ratios_cont\", \"ratios_tr12_cont\", \"ratios_tr21_cont\", \"qgram\", \"qgram_tr21\",\n",
    "       \"coltdetc_n1\", \"coltdetc_n2\", \"n_intersect\", \"upper_count1\", \"upper_count2\", \"num_punct1\", \"num_punct2\", \n",
    "       \"feat1\", \"feat2\", \"feat3\", \"feat4\", \"feat5\", \"feat6\", \"total employee estimate_n1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer()\n",
    "tfidf2 = TfidfVectorizer()\n",
    "tfidf3 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs1 = tfidf1.fit_transform(train['name_1'])\n",
    "vecs2 = tfidf2.fit_transform(train['name_2'])\n",
    "vecs3 = tfidf3.fit_transform(train['intersections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sp.hstack((train[FTS], vecs1, vecs2))\n",
    "y = train['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sp.hstack((X, train_sparse_lev_feats, train_sparse_norm_lev_feats, train_sparse_ratio_feats, train_sparse_junk_feats, vecs3, enc.transform(train[['industry_n1', 'industry_n2']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sp.hstack((test[FTS], tfidf1.transform(test['name_1']), tfidf2.transform(test['name_2'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sp.hstack((X_test, \n",
    "                    test_sparse_lev_feats,\n",
    "                    test_sparse_norm_lev_feats,\n",
    "                    test_sparse_ratio_feats,\n",
    "                    test_sparse_junk_feats,\n",
    "                    tfidf3.transform(test['intersections']),\n",
    "                    enc.transform(test[['industry_n1', 'industry_n2']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(DATA_DIR.joinpath(\"sample_submission.csv\"), index_col=\"pair_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub[\"is_duplicate\"] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv(DATA_DIR.joinpath(\"baseline_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sibur",
   "language": "python",
   "name": "sibur"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
